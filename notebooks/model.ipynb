{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhonlucas/Documents/projects/translation-libras-to-ptbr/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('../data/libras_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>subject</th>\n",
       "      <th>interpretation</th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "      <th>example_libras_sentence</th>\n",
       "      <th>grammar_class</th>\n",
       "      <th>word_origin</th>\n",
       "      <th>video_link</th>\n",
       "      <th>image_link</th>\n",
       "      <th>hand_image_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Primeira letra do alfabeto da língua portuguesa; sinal gráfico elementar com que se representam os vocábulos na língua escrita.</td>\n",
       "      <td>Invente qualquer palavra que comece com a letra A.</td>\n",
       "      <td>VOCÊ INVENTAR QUALQUER PALAVRA COMEÇAR A.</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/aSm_Prog001.mp4</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/aSm_Prog001.jpg</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg01.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABACATE</td>\n",
       "      <td>FRUTA</td>\n",
       "      <td>O fruto do abacateiro. Comestível, tem a polpa amarelada e macia. É consumido puro, com açúcar, em pratos salgados ou em vitaminas.</td>\n",
       "      <td>Você gosta de abacate com leite?</td>\n",
       "      <td>VOCÊ GOSTAR ABACATE LEITE JUNTO?</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/abacateSm_Prog001.mp4</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/abacateSm_Prog001.jpg</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg53a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACAXI</td>\n",
       "      <td>FRUTA</td>\n",
       "      <td>Fruta de casca grossa e áspera. Sua polpa pode ser consumida pura, em forma de sucos, doces e sorvetes.</td>\n",
       "      <td>Hoje tomei suco de abacaxi, ele estava ácido.</td>\n",
       "      <td>HOJE S-U-C-O ABACAXI BEBER ÁCID@.</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/abacaxiSm_Prog001.mp4</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/abacaxiSm_Prog001.jpg</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg47.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABAFAR</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Cobrir ou fechar, para manter o calor.</td>\n",
       "      <td>Se você quer abafar seu quarto, é melhor fechar tudo.</td>\n",
       "      <td>S-I VOCÊ QUERER QUARTO SE@ ABAFAR A-R? MELHOR FECHAR-TUDO.</td>\n",
       "      <td>VERBO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/abafarSm_Prog001.mp4</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/abafarSm_Prog001.jpg</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg07.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABAIXO</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Lugar, posição ou situação inferior, em relação a outros de nível mais elevado.</td>\n",
       "      <td>Não é no primeiro apartamento abaixo, é no segundo.</td>\n",
       "      <td>APARTAMENTO PRIMEIR@ NÃO SEGUND@ ABAIXO.</td>\n",
       "      <td>ADV.</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/abaixoSm_Prog001.mp4</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/abaixoSm_Prog001.jpg</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg62.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word subject  \\\n",
       "0        A  NENHUM   \n",
       "1  ABACATE   FRUTA   \n",
       "2  ABACAXI   FRUTA   \n",
       "3   ABAFAR  NENHUM   \n",
       "4   ABAIXO  NENHUM   \n",
       "\n",
       "                                                                                                                        interpretation  \\\n",
       "0      Primeira letra do alfabeto da língua portuguesa; sinal gráfico elementar com que se representam os vocábulos na língua escrita.   \n",
       "1  O fruto do abacateiro. Comestível, tem a polpa amarelada e macia. É consumido puro, com açúcar, em pratos salgados ou em vitaminas.   \n",
       "2                              Fruta de casca grossa e áspera. Sua polpa pode ser consumida pura, em forma de sucos, doces e sorvetes.   \n",
       "3                                                                                               Cobrir ou fechar, para manter o calor.   \n",
       "4                                                      Lugar, posição ou situação inferior, em relação a outros de nível mais elevado.   \n",
       "\n",
       "                             example_portuguese_sentence  \\\n",
       "0     Invente qualquer palavra que comece com a letra A.   \n",
       "1                       Você gosta de abacate com leite?   \n",
       "2          Hoje tomei suco de abacaxi, ele estava ácido.   \n",
       "3  Se você quer abafar seu quarto, é melhor fechar tudo.   \n",
       "4    Não é no primeiro apartamento abaixo, é no segundo.   \n",
       "\n",
       "                                      example_libras_sentence grammar_class  \\\n",
       "0                   VOCÊ INVENTAR QUALQUER PALAVRA COMEÇAR A.   SUBSTANTIVO   \n",
       "1                            VOCÊ GOSTAR ABACATE LEITE JUNTO?   SUBSTANTIVO   \n",
       "2                           HOJE S-U-C-O ABACAXI BEBER ÁCID@.   SUBSTANTIVO   \n",
       "3  S-I VOCÊ QUERER QUARTO SE@ ABAFAR A-R? MELHOR FECHAR-TUDO.         VERBO   \n",
       "4                    APARTAMENTO PRIMEIR@ NÃO SEGUND@ ABAIXO.          ADV.   \n",
       "\n",
       "  word_origin  \\\n",
       "0    Nacional   \n",
       "1    Nacional   \n",
       "2    Nacional   \n",
       "3    Nacional   \n",
       "4    Nacional   \n",
       "\n",
       "                                                                                        video_link  \\\n",
       "0        https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/aSm_Prog001.mp4   \n",
       "1  https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/abacateSm_Prog001.mp4   \n",
       "2  https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/abacaxiSm_Prog001.mp4   \n",
       "3   https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/abafarSm_Prog001.mp4   \n",
       "4   https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/videos/abaixoSm_Prog001.mp4   \n",
       "\n",
       "                                                                                        image_link  \\\n",
       "0        https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/aSm_Prog001.jpg   \n",
       "1  https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/abacateSm_Prog001.jpg   \n",
       "2  https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/abacaxiSm_Prog001.jpg   \n",
       "3   https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/abafarSm_Prog001.jpg   \n",
       "4   https://www.ines.gov.br/dicionario-de-libras/public/media/palavras/images/abaixoSm_Prog001.jpg   \n",
       "\n",
       "                                                           hand_image_link  \n",
       "0   https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg01.jpg  \n",
       "1  https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg53a.jpg  \n",
       "2   https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg47.jpg  \n",
       "3   https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg07.jpg  \n",
       "4   https://www.ines.gov.br/dicionario-de-libras/public/media/mao/cg62.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 5 rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "      <th>example_libras_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invente qualquer palavra que comece com a letra A.</td>\n",
       "      <td>VOCÊ INVENTAR QUALQUER PALAVRA COMEÇAR A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Você gosta de abacate com leite?</td>\n",
       "      <td>VOCÊ GOSTAR ABACATE LEITE JUNTO?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoje tomei suco de abacaxi, ele estava ácido.</td>\n",
       "      <td>HOJE S-U-C-O ABACAXI BEBER ÁCID@.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Se você quer abafar seu quarto, é melhor fechar tudo.</td>\n",
       "      <td>S-I VOCÊ QUERER QUARTO SE@ ABAFAR A-R? MELHOR FECHAR-TUDO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Não é no primeiro apartamento abaixo, é no segundo.</td>\n",
       "      <td>APARTAMENTO PRIMEIR@ NÃO SEGUND@ ABAIXO.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             example_portuguese_sentence  \\\n",
       "0     Invente qualquer palavra que comece com a letra A.   \n",
       "1                       Você gosta de abacate com leite?   \n",
       "2          Hoje tomei suco de abacaxi, ele estava ácido.   \n",
       "3  Se você quer abafar seu quarto, é melhor fechar tudo.   \n",
       "4    Não é no primeiro apartamento abaixo, é no segundo.   \n",
       "\n",
       "                                      example_libras_sentence  \n",
       "0                   VOCÊ INVENTAR QUALQUER PALAVRA COMEÇAR A.  \n",
       "1                            VOCÊ GOSTAR ABACATE LEITE JUNTO?  \n",
       "2                           HOJE S-U-C-O ABACAXI BEBER ÁCID@.  \n",
       "3  S-I VOCÊ QUERER QUARTO SE@ ABAFAR A-R? MELHOR FECHAR-TUDO.  \n",
       "4                    APARTAMENTO PRIMEIR@ NÃO SEGUND@ ABAIXO.  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['example_portuguese_sentence', 'example_libras_sentence']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove characters that are not letters from the sentences in Libras\n",
    "data['example_libras_sentence'] = data['example_libras_sentence'].str.replace(\"@\", \"o\")\n",
    "data['example_libras_sentence'] = data['example_libras_sentence'].str.replace(\"-\", \"\")\n",
    "data['example_libras_sentence'] = data['example_libras_sentence'].str.lower()\n",
    "data['example_portuguese_sentence'] = data['example_portuguese_sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "      <th>example_libras_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>invente qualquer palavra que comece com a letra a.</td>\n",
       "      <td>você inventar qualquer palavra começar a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>você gosta de abacate com leite?</td>\n",
       "      <td>você gostar abacate leite junto?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hoje tomei suco de abacaxi, ele estava ácido.</td>\n",
       "      <td>hoje suco abacaxi beber ácido.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>se você quer abafar seu quarto, é melhor fechar tudo.</td>\n",
       "      <td>si você querer quarto seo abafar ar? melhor fechartudo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>não é no primeiro apartamento abaixo, é no segundo.</td>\n",
       "      <td>apartamento primeiro não segundo abaixo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>os surdos fizeram um abaixo-assinado pedindo mais empregos ao governo.</td>\n",
       "      <td>surdo abaixoassinado pedir governo emprego mais.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a lâmpada do abajur queimou.</td>\n",
       "      <td>coisacônicalâmpada lâmpadaqueimar .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>está vendo aquela velha se abanando? ela é avó da minha amiga.</td>\n",
       "      <td>2solhar3s velho abanarleque lá(me) vovo amigo(md).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>o carro velho foi abandonado naquela garagem.</td>\n",
       "      <td>aquelo caragem carro velho abandonar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>coitada dessa criança tão bonita dormindo na rua. como seus pais tiveram coragem de abandoná-la?!</td>\n",
       "      <td>criança bonito dormir rua coitado! pai mãe delo abandonar coragem?!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         example_portuguese_sentence  \\\n",
       "0                                                 invente qualquer palavra que comece com a letra a.   \n",
       "1                                                                   você gosta de abacate com leite?   \n",
       "2                                                      hoje tomei suco de abacaxi, ele estava ácido.   \n",
       "3                                              se você quer abafar seu quarto, é melhor fechar tudo.   \n",
       "4                                                não é no primeiro apartamento abaixo, é no segundo.   \n",
       "5                             os surdos fizeram um abaixo-assinado pedindo mais empregos ao governo.   \n",
       "6                                                                       a lâmpada do abajur queimou.   \n",
       "7                                     está vendo aquela velha se abanando? ela é avó da minha amiga.   \n",
       "8                                                      o carro velho foi abandonado naquela garagem.   \n",
       "9  coitada dessa criança tão bonita dormindo na rua. como seus pais tiveram coragem de abandoná-la?!   \n",
       "\n",
       "                                               example_libras_sentence  \n",
       "0                            você inventar qualquer palavra começar a.  \n",
       "1                                     você gostar abacate leite junto?  \n",
       "2                                       hoje suco abacaxi beber ácido.  \n",
       "3              si você querer quarto seo abafar ar? melhor fechartudo.  \n",
       "4                             apartamento primeiro não segundo abaixo.  \n",
       "5                     surdo abaixoassinado pedir governo emprego mais.  \n",
       "6                                  coisacônicalâmpada lâmpadaqueimar .  \n",
       "7                   2solhar3s velho abanarleque lá(me) vovo amigo(md).  \n",
       "8                                aquelo caragem carro velho abandonar.  \n",
       "9  criança bonito dormir rua coitado! pai mãe delo abandonar coragem?!  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['example_portuguese_sentence', 'example_libras_sentence']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['example_portuguese_sentence', 'example_libras_sentence']].to_csv('../data/libras_dictionary_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with manual corrections\n",
    "data_cleaned = pd.read_csv('../data/libras_dictionary_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "      <th>example_libras_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>invente qualquer palavra que comece com a letra a.</td>\n",
       "      <td>você inventar qualquer palavra começar a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>você gosta de abacate com leite?</td>\n",
       "      <td>você gostar abacate leite junto?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hoje tomei suco de abacaxi, ele estava ácido.</td>\n",
       "      <td>hoje suco abacaxi beber ácido.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>se você quer abafar seu quarto, é melhor fechar tudo.</td>\n",
       "      <td>si você querer quarto seo abafar ar? melhor fechartudo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>não é no primeiro apartamento abaixo, é no segundo.</td>\n",
       "      <td>apartamento primeiro não segundo abaixo.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             example_portuguese_sentence  \\\n",
       "0     invente qualquer palavra que comece com a letra a.   \n",
       "1                       você gosta de abacate com leite?   \n",
       "2          hoje tomei suco de abacaxi, ele estava ácido.   \n",
       "3  se você quer abafar seu quarto, é melhor fechar tudo.   \n",
       "4    não é no primeiro apartamento abaixo, é no segundo.   \n",
       "\n",
       "                                   example_libras_sentence  \n",
       "0                você inventar qualquer palavra começar a.  \n",
       "1                         você gostar abacate leite junto?  \n",
       "2                           hoje suco abacaxi beber ácido.  \n",
       "3  si você querer quarto seo abafar ar? melhor fechartudo.  \n",
       "4                 apartamento primeiro não segundo abaixo.  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optmi\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from pprint import pprint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "from torch.nn import (TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer)\n",
    "import spacy\n",
    "\n",
    "# Seeding for reproducible results everytime\n",
    "SEED = 777\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = AutoModelForPreTraining.from_pretrained('neuralmind/bert-large-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased', do_lower_case=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Preprocessing\n",
    "\n",
    "Let's see some of the process it can do,\n",
    "\n",
    "* Train/ Valid/ Test Split: partition your data into a specified train/ valid/ test set.\n",
    "\n",
    "* File Loading: load the text corpus of various formats (.txt,.json,.csv).\n",
    "\n",
    "* Tokenization: breaking sentences into list of words.\n",
    "\n",
    "* Vocab: Generate a list of vocabulary from the text corpus.\n",
    "\n",
    "* Words to Integer Mapper: Map words into integer numbers for the entire corpus and vice versa.\n",
    "\n",
    "* Word Vector: Convert a word from higher dimension to lower dimension (Word Embedding).\n",
    "\n",
    "* Batching: Generate batches of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy portuguese tokenizer\n",
    "spacy_pt = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove characters that are not letters\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ú.!?]+\", ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "\n",
    "    # Remove extra spaces at the beginning and end of the sentence\n",
    "    text = text.strip()\n",
    "\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(text):\n",
    "\n",
    "    text = clean_text(text)\n",
    "\n",
    "    return [tok.text for tok in spacy_pt.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "libras_sentences_source = Field(\n",
    "    tokenize = spacy_tokenizer,\n",
    "    lower=True,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "portuguese_sentences_target = Field(\n",
    "    tokenize = spacy_tokenizer,\n",
    "    lower=True,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset(path='../data/data_svo.csv', format='csv', fields=[('trg', portuguese_sentences_target), ('src', libras_sentences_source)], skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = dataset.split(split_ratio=[0.8, 0.1, 0.1], random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 43666\n",
      "Number of validation examples: 5458\n",
      "Number of testing examples: 5458\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tain sentence example - Libras  ['nós', 'serer', 'voz', 'pessoa']\n",
      "Tain sentence example - Portuguese  ['nós', 'seremos', 'a', 'voz', 'destas', 'pessoas', 'sem', 'voz', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tain sentence example - Libras \", train_data.examples[0].src)\n",
    "print(\"Tain sentence example - Portuguese \", train_data.examples[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "libras_sentences_source.build_vocab(train_data, min_freq=2)\n",
    "portuguese_sentences_target.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (libras) vocabulary: 8504\n",
      "Unique tokens in target (portuguse) vocabulary: 12778\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (libras) vocabulary: {len(libras_sentences_source.vocab)}\")\n",
    "print(f\"Unique tokens in target (portuguse) vocabulary: {len(portuguese_sentences_target.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([20, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([19, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([23, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([20, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([19, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([19, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([20, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([19, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([24, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([22, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([19, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([20, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([19, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([19, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([19, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([22, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([24, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([18, 128])\n",
      "torch.Size([11, 128])\n",
      "torch.Size([21, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([4, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([9, 128])\n",
      "torch.Size([17, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([13, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([14, 128])\n",
      "torch.Size([7, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([6, 128])\n",
      "torch.Size([15, 128])\n",
      "torch.Size([14, 18])\n",
      "torch.Size([24, 18])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(batch.src.shape)\n",
    "    print(batch.trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(libras_sentences_source.vocab)\n",
    "OUTPUT_DIM = len(portuguese_sentences_target.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8504, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(12778, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=12778, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,359,722 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optmi.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = portuguese_sentences_target.vocab.stoi[portuguese_sentences_target.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 32s\n",
      "\tTrain Loss: 5.663 | Train PPL: 287.909\n",
      "\t Val. Loss: 5.356 |  Val. PPL: 211.838\n",
      "Epoch: 02 | Time: 0m 32s\n",
      "\tTrain Loss: 5.245 | Train PPL: 189.606\n",
      "\t Val. Loss: 5.117 |  Val. PPL: 166.808\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "N_EPOCHS = 2\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'slt_libras_v2.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('slt_libras_v2.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.126490005227023"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.126 | Test PPL: 168.425 |\n"
     ]
    }
   ],
   "source": [
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, libras, portuguese, device, max_length=50):\n",
    "\n",
    "    if type(sentence) == str:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    tokens.insert(0, libras.init_token)\n",
    "    tokens.append(libras.eos_token)\n",
    "    text_to_indices = [libras.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [portuguese.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == portuguese.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [portuguese.vocab.itos[idx] for idx in outputs]\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8504, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(12778, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=12778, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence:  ['ele', 'tipo', 'aprender', 'lição']\n",
      "Correct translation:  ['que', 'tipo', 'de', 'lições', 'ela', 'vai', 'aprender', 'com', 'eles', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Test sentence: \", test_data[150].src)\n",
    "print(\"Correct translation: \", test_data[150].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\"eu férias julho planejar viajar europa.\"]\n",
    "correct_sentence = [\"eu tenho férias em julho e vou planejar uma viagem `a europa.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence:  ['e', 'é', 'o', 'de', 'de', 'de', '.', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Translated sentence: \", translate_sentence(model, test_sentence, libras_sentences_source, portuguese_sentences_target, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  tornar ponto virada também\n",
      "target sentence:  e se tornou um ponto de virada para mim também .\n",
      "torch.Size([13, 1])\n",
      "Shape: torch.Size([13, 1, 12778])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e é o que é a de . . <eos> <eos> <eos>'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_idx = 1496\n",
    "example = val_data.examples[example_idx]\n",
    "print('source sentence: ', ' '.join(example.src))\n",
    "print('target sentence: ', ' '.join(example.trg))\n",
    "\n",
    "src_tensor = libras_sentences_source.process([example.src]).to(device)\n",
    "trg_tensor = portuguese_sentences_target.process([example.trg]).to(device)\n",
    "print(trg_tensor.shape)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(src_tensor, trg_tensor, teacher_forcing_ratio=0.1)\n",
    "\n",
    "print(f\"Shape: {outputs.shape}\")\n",
    "\n",
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "' '.join([portuguese_sentences_target.vocab.itos[idx] for idx in output_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448d780881a940d49ab191b2d1b463a8c02ae47a3b8017bde1517a5fe99f211"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
