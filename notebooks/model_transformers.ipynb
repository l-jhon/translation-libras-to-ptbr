{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "portuguese_df_ted_talk = pd.read_csv('../data/pt_br_tedtalk.txt', sep='\\t', header=None, names=['example_portuguese_sentence'])\n",
    "portuguese_df_neulab = pd.read_csv('../data/pt_br_neulab.txt', sep='\\t', header=None, names=['example_portuguese_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Em meados do século 16, os italianos ficavam e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Porém, esse talento tinha um preço alto.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Para evitar que a voz desses cantores se quebr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conhecidos como \"castrati\", a voz leve e angel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O interrompimento do desenvolvimento vocal pod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         example_portuguese_sentence\n",
       "0  Em meados do século 16, os italianos ficavam e...\n",
       "1           Porém, esse talento tinha um preço alto.\n",
       "2  Para evitar que a voz desses cantores se quebr...\n",
       "3  Conhecidos como \"castrati\", a voz leve e angel...\n",
       "4  O interrompimento do desenvolvimento vocal pod..."
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portuguese_df_ted_talk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pensei em ler os meus poemas que tem relação c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu fiquei meio surpreso ao descobrir quantos, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O primeiro é dedicado a Spencer, e sua avó, qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meu poema se chama \"\" Sujeira \"\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Minha avó está lavando minha boca com sabão; m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         example_portuguese_sentence\n",
       "0  Pensei em ler os meus poemas que tem relação c...\n",
       "1  Eu fiquei meio surpreso ao descobrir quantos, ...\n",
       "2  O primeiro é dedicado a Spencer, e sua avó, qu...\n",
       "3                  Meu poema se chama \"\" Sujeira \"\".\n",
       "4  Minha avó está lavando minha boca com sabão; m..."
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portuguese_df_neulab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with manual corrections\n",
    "data_cleaned = pd.read_csv('../data/libras_dictionary.csv')\n",
    "data_cleaned[\"source\"] = \"ines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>subject</th>\n",
       "      <th>interpretation</th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "      <th>example_libras_sentence</th>\n",
       "      <th>grammar_class</th>\n",
       "      <th>word_origin</th>\n",
       "      <th>video_link</th>\n",
       "      <th>image_link</th>\n",
       "      <th>hand_image_link</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Primeira letra do alfabeto da língua portugues...</td>\n",
       "      <td>Invente qualquer palavra que comece com a letr...</td>\n",
       "      <td>VOCÊ INVENTAR QUALQUER PALAVRA COMEÇAR A.</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABACATE</td>\n",
       "      <td>FRUTA</td>\n",
       "      <td>O fruto do abacateiro. Comestível, tem a polpa...</td>\n",
       "      <td>Você gosta de abacate com leite?</td>\n",
       "      <td>VOCÊ GOSTAR ABACATE LEITE JUNTO?</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACAXI</td>\n",
       "      <td>FRUTA</td>\n",
       "      <td>Fruta de casca grossa e áspera. Sua polpa pode...</td>\n",
       "      <td>Hoje tomei suco de abacaxi, ele estava ácido.</td>\n",
       "      <td>HOJE S-U-C-O ABACAXI BEBER ÁCID@.</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABAFAR</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Cobrir ou fechar, para manter o calor.</td>\n",
       "      <td>Se você quer abafar seu quarto, é melhor fecha...</td>\n",
       "      <td>S-I VOCÊ QUERER QUARTO SE@ ABAFAR A-R? MELHOR ...</td>\n",
       "      <td>VERBO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABAIXO</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Lugar, posição ou situação inferior, em relaçã...</td>\n",
       "      <td>Não é no primeiro apartamento abaixo, é no seg...</td>\n",
       "      <td>APARTAMENTO PRIMEIR@ NÃO SEGUND@ ABAIXO.</td>\n",
       "      <td>ADV.</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word subject                                     interpretation  \\\n",
       "0        A  NENHUM  Primeira letra do alfabeto da língua portugues...   \n",
       "1  ABACATE   FRUTA  O fruto do abacateiro. Comestível, tem a polpa...   \n",
       "2  ABACAXI   FRUTA  Fruta de casca grossa e áspera. Sua polpa pode...   \n",
       "3   ABAFAR  NENHUM             Cobrir ou fechar, para manter o calor.   \n",
       "4   ABAIXO  NENHUM  Lugar, posição ou situação inferior, em relaçã...   \n",
       "\n",
       "                         example_portuguese_sentence  \\\n",
       "0  Invente qualquer palavra que comece com a letr...   \n",
       "1                   Você gosta de abacate com leite?   \n",
       "2      Hoje tomei suco de abacaxi, ele estava ácido.   \n",
       "3  Se você quer abafar seu quarto, é melhor fecha...   \n",
       "4  Não é no primeiro apartamento abaixo, é no seg...   \n",
       "\n",
       "                             example_libras_sentence grammar_class  \\\n",
       "0          VOCÊ INVENTAR QUALQUER PALAVRA COMEÇAR A.   SUBSTANTIVO   \n",
       "1                   VOCÊ GOSTAR ABACATE LEITE JUNTO?   SUBSTANTIVO   \n",
       "2                  HOJE S-U-C-O ABACAXI BEBER ÁCID@.   SUBSTANTIVO   \n",
       "3  S-I VOCÊ QUERER QUARTO SE@ ABAFAR A-R? MELHOR ...         VERBO   \n",
       "4           APARTAMENTO PRIMEIR@ NÃO SEGUND@ ABAIXO.          ADV.   \n",
       "\n",
       "  word_origin                                         video_link  \\\n",
       "0    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "1    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "2    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "3    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "4    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "\n",
       "                                          image_link  \\\n",
       "0  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "1  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "2  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "3  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "4  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "\n",
       "                                     hand_image_link source  \n",
       "0  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "1  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "2  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "3  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "4  https://www.ines.gov.br/dicionario-de-libras/p...   ines  "
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean words: 8.660837950138504\n",
      "Mean characters: 47.44719529085872\n"
     ]
    }
   ],
   "source": [
    "# Get the mean size of the sentences\n",
    "\n",
    "print(f\"Mean words: {data_cleaned['example_portuguese_sentence'].str.split().str.len().mean()}\")\n",
    "print(f\"Mean characters: {data_cleaned['example_portuguese_sentence'].str.len().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the sentences with less than 50 characters\n",
    "\n",
    "portuguese_df_ted_talk = portuguese_df_ted_talk[(portuguese_df_ted_talk['example_portuguese_sentence'].str.len() <= 50) & (portuguese_df_ted_talk['example_portuguese_sentence'].str.split().str.len() >= 8)]\n",
    "portuguese_df_ted_talk[\"source\"] = \"opus_nlpl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the sentences with less than 50 characters\n",
    "\n",
    "portuguese_df_neulab = portuguese_df_neulab[(portuguese_df_neulab['example_portuguese_sentence'].str.len() <= 50) & (portuguese_df_neulab['example_portuguese_sentence'].str.split().str.len() >= 8)]\n",
    "portuguese_df_neulab[\"source\"] = \"opus_nlpl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    data_cleaned[[\"example_portuguese_sentence\", \"source\"]], \n",
    "    portuguese_df_neulab[[\"example_portuguese_sentence\", \"source\"]],\n",
    "    portuguese_df_ted_talk[[\"example_portuguese_sentence\", \"source\"]]],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invente qualquer palavra que comece com a letr...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Você gosta de abacate com leite?</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoje tomei suco de abacaxi, ele estava ácido.</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Se você quer abafar seu quarto, é melhor fecha...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Não é no primeiro apartamento abaixo, é no seg...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Se o freio estiver com problema, é perigoso o ...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Aquele homem tem muita acne no rosto.</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A janela da minha casa é de aço, difícil de qu...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Aquele homem açoitou o cavalo para que acelera...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Meu açoite está velho.</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          example_portuguese_sentence source\n",
       "0   Invente qualquer palavra que comece com a letr...   ines\n",
       "1                    Você gosta de abacate com leite?   ines\n",
       "2       Hoje tomei suco de abacaxi, ele estava ácido.   ines\n",
       "3   Se você quer abafar seu quarto, é melhor fecha...   ines\n",
       "4   Não é no primeiro apartamento abaixo, é no seg...   ines\n",
       "..                                                ...    ...\n",
       "95  Se o freio estiver com problema, é perigoso o ...   ines\n",
       "96              Aquele homem tem muita acne no rosto.   ines\n",
       "97  A janela da minha casa é de aço, difícil de qu...   ines\n",
       "98  Aquele homem açoitou o cavalo para que acelera...   ines\n",
       "99                             Meu açoite está velho.   ines\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54581, 2)"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optmi\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from pprint import pprint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import spacy\n",
    "\n",
    "# Seeding for reproducible results everytime\n",
    "SEED = 777\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Preprocessing\n",
    "\n",
    "Let's see some of the process it can do,\n",
    "\n",
    "* Train/ Valid/ Test Split: partition your data into a specified train/ valid/ test set.\n",
    "\n",
    "* File Loading: load the text corpus of various formats (.txt,.json,.csv).\n",
    "\n",
    "* Tokenization: breaking sentences into list of words.\n",
    "\n",
    "* Vocab: Generate a list of vocabulary from the text corpus.\n",
    "\n",
    "* Words to Integer Mapper: Map words into integer numbers for the entire corpus and vice versa.\n",
    "\n",
    "* Word Vector: Convert a word from higher dimension to lower dimension (Word Embedding).\n",
    "\n",
    "* Batching: Generate batches of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy portuguese tokenizer\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(clean_text(data['example_portuguese_sentence'][15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o o DET det\n",
      "médico médico NOUN nsubj\n",
      "me eu PRON obj\n",
      "mandou mandar VERB ROOT\n",
      "tirar tirar VERB xcomp\n",
      "raio raio NOUN obj\n",
      "x x VERB obj\n",
      "do de o ADP case\n",
      "abdômen Abdômen NOUN iobj\n",
      ". . PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "médico NOUN nsubj\n",
      "eu PRON obj\n",
      "mandar VERB ROOT\n",
      "tirar VERB xcomp\n",
      "raio NOUN obj\n",
      "x VERB obj\n",
      "Abdômen NOUN iobj\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == 'VERB'\\\n",
    "        or token.pos_ == 'NOUN' \\\n",
    "        or token.pos_ == 'ADJ' \\\n",
    "        or token.pos_ == 'ADV' \\\n",
    "        or token.pos_ == 'NUM' \\\n",
    "        or token.pos_ == 'PROPN' \\\n",
    "        or token.pos_ == 'PRON' \\\n",
    "        or token.pos_ == 'SCONJ':\n",
    "        print(token.lemma_, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize sentences in subject, verb, object\n",
    "\n",
    "def get_subjects(doc):\n",
    "\n",
    "    subjects = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            subjects.append(token.lemma_)\n",
    "    return subjects\n",
    "\n",
    "def get_verbs(doc):\n",
    "    \n",
    "    verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            verbs.append(token.lemma_)\n",
    "    return verbs\n",
    "\n",
    "def get_objects(doc):\n",
    "\n",
    "    objects = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'obj' or token.dep_ == 'dobj' or token.dep_ == 'iobj':\n",
    "            objects.append(token.lemma_)\n",
    "    return objects\n",
    "\n",
    "\n",
    "def get_nouns(doc):\n",
    "\n",
    "    nouns = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            nouns.append(token.lemma_)\n",
    "    return nouns\n",
    "\n",
    "def get_adjectives(doc):\n",
    "\n",
    "    adjectives = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            adjectives.append(token.lemma_)\n",
    "    return adjectives\n",
    "\n",
    "def get_adverbs(doc):\n",
    "\n",
    "    adverbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADV':\n",
    "            adverbs.append(token.lemma_)\n",
    "    return adverbs\n",
    "\n",
    "def get_numbers(doc):\n",
    "\n",
    "    numbers = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NUM':\n",
    "            numbers.append(token.lemma_)\n",
    "    return numbers\n",
    "\n",
    "\n",
    "def get_conjunctions(doc):\n",
    "\n",
    "    conjunctions = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'SCONJ':\n",
    "            conjunctions.append(token.lemma_)\n",
    "    return conjunctions\n",
    "\n",
    "def get_pronouns(doc):\n",
    "\n",
    "    pronouns = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            pronouns.append(token.lemma_)\n",
    "    return pronouns\n",
    "\n",
    "def get_proper_nouns(doc):\n",
    "\n",
    "    proper_nouns = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            proper_nouns.append(token.lemma_)\n",
    "    return proper_nouns\n",
    "\n",
    "\n",
    "def organize_sentence_svo(doc):\n",
    "    sentence = []\n",
    "    sentence.append(get_subjects(doc))\n",
    "    sentence.append(get_verbs(doc))\n",
    "    sentence.append(get_objects(doc))\n",
    "    sentence.append(get_nouns(doc))\n",
    "    sentence.append(get_adjectives(doc))\n",
    "    sentence.append(get_adverbs(doc))\n",
    "    sentence.append(get_numbers(doc))\n",
    "    #sentence.append(get_conjunctions(doc))\n",
    "    #sentence.append(get_pronouns(doc))\n",
    "    #sentence.append(get_proper_nouns(doc))\n",
    "\n",
    "    # remove empty lists\n",
    "\n",
    "    sentence = [list(set(x)) for x in sentence if x != []]\n",
    "\n",
    "    # Join the lists\n",
    "\n",
    "    sentence = [item for sublist in sentence for item in sublist]\n",
    "\n",
    "    # Rmove duplicate words and join the sentence, keeping the order\n",
    "\n",
    "    sentence = ' '.join(list(dict.fromkeys(sentence)))\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aborrecer valer pena não'"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organize_sentence_svo(nlp(data['example_portuguese_sentence'][30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataset\n",
    "\n",
    "data['example_libras_sentence_svo'] = data['example_portuguese_sentence'].apply(lambda x: organize_sentence_svo(nlp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"example_portuguese_sentence\", \"example_libras_sentence_svo\"]].to_csv('../data/data_svo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove characters that are not letters\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ú.!?]+\", ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "\n",
    "    # Remove extra spaces at the beginning and end of the sentence\n",
    "    text = text.strip()\n",
    "\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(text):\n",
    "\n",
    "    text = clean_text(text)\n",
    "\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "libras_sentences_source = Field(\n",
    "    tokenize = spacy_tokenizer,\n",
    "    lower=True,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "portuguese_sentences_target = Field(\n",
    "    tokenize = spacy_tokenizer,\n",
    "    lower=True,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset(path='../data/data_svo.csv', format='csv', fields=[('trg', portuguese_sentences_target), ('src', libras_sentences_source)], skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = dataset.split(split_ratio=[0.8, 0.1, 0.1], random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 43666\n",
      "Number of validation examples: 5458\n",
      "Number of testing examples: 5458\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tain sentence example - Libras  ['que', 'resposta', 'faltar']\n",
      "Tain sentence example - Portuguese  ['e', 'a', 'resposta', 'é', 'o', 'que', 'está', 'faltando', '?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tain sentence example - Libras \", train_data.examples[3].src)\n",
    "print(\"Tain sentence example - Portuguese \", train_data.examples[3].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "libras_sentences_source.build_vocab(train_data, min_freq=2)\n",
    "portuguese_sentences_target.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (libras) vocabulary: 8504\n",
      "Unique tokens in target (portuguse) vocabulary: 12778\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (libras) vocabulary: {len(libras_sentences_source.vocab)}\")\n",
    "print(f\"Unique tokens in target (portuguse) vocabulary: {len(portuguese_sentences_target.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - source batch\n",
    "    Layer : \n",
    "        source batch -> Embedding -> LSTM\n",
    "    Output :\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    input_dim : int\n",
    "        Input dimension, should equal to the source vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, emb_dim: int, hid_dim: int, n_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src_batch: torch.LongTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_batch : 2d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [sent len, batch size].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden, cell : 3d torch.LongTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(src_batch) # [sent len, batch size, emb dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs -> [sent len, batch size, hidden dim * n directions]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - first token in the target batch\n",
    "        - LSTM hidden state from the encoder\n",
    "        - LSTM cell state from the encoder\n",
    "    Layer :\n",
    "        target batch -> Embedding -- \n",
    "                                   |\n",
    "        encoder hidden state ------|--> LSTM -> Linear\n",
    "                                   |\n",
    "        encoder cell state   -------\n",
    "        \n",
    "    Output :\n",
    "        - prediction\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    output : int\n",
    "        Output dimension, should equal to the target vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int, emb_dim: int, hid_dim: int, n_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, trg: torch.LongTensor, hidden: torch.FloatTensor, cell: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trg : 1d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [batch size].\n",
    "            \n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : 2d torch.LongTensor\n",
    "            For each token in the batch, the predicted target vobulary.\n",
    "            Shape [batch size, output dim]\n",
    "\n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        # [1, batch size, emb dim], the 1 serves as sent len\n",
    "        embedded = self.embedding(trg.unsqueeze(0))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.out(outputs.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            'Hidden dimensions of encoder and decoder must be equal!'\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'Encoder and decoder must have equal number of layers!'\n",
    "\n",
    "    def forward(self, src_batch: torch.LongTensor, trg_batch: torch.LongTensor,\n",
    "                teacher_forcing_ratio: float=0.5):\n",
    "\n",
    "        max_len, batch_size = trg_batch.shape\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "        hidden, cell = self.encoder(src_batch)\n",
    "\n",
    "        trg = trg_batch[0]\n",
    "        for i in range(1, max_len):\n",
    "            prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "            outputs[i] = prediction\n",
    "\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                trg = trg_batch[i]\n",
    "            else:\n",
    "                trg = prediction.argmax(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([26, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([22, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([22, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([24, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([23, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([22, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([24, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([23, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([24, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([22, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([24, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([21, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([4, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([14, 14])\n",
      "torch.Size([27, 14])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([5, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([14, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(batch.src.shape)\n",
    "    print(batch.trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(libras_sentences_source.vocab)\n",
    "OUTPUT_DIM = len(portuguese_sentences_target.vocab)\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = libras_sentences_source.vocab.stoi[libras_sentences_source.pad_token]\n",
    "TRG_PAD_IDX = portuguese_sentences_target.vocab.stoi[portuguese_sentences_target.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,192,822 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6207, 32)\n",
       "    (rnn): LSTM(32, 64, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(9046, 32)\n",
       "    (rnn): LSTM(32, 64, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=64, out_features=9046, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq, iterator, optimizer, criterion):\n",
    "    seq2seq.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = seq2seq(batch.src, batch.trg)\n",
    "\n",
    "        # 1. as mentioned in the seq2seq section, we will\n",
    "        # cut off the first element when performing the evaluation\n",
    "        # 2. the loss function only works on 2d inputs\n",
    "        # with 1d targets we need to flatten each of them\n",
    "        outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        trg_flatten = batch.trg[1:].view(-1)\n",
    "        loss = criterion(outputs_flatten, trg_flatten)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(seq2seq, iterator, criterion):\n",
    "    seq2seq.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # turn off teacher forcing\n",
    "            outputs = seq2seq(batch.src, batch.trg, teacher_forcing_ratio=0) \n",
    "\n",
    "            # trg = [trg sent len, batch size]\n",
    "            # output = [trg sent len, batch size, output dim]\n",
    "            outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            trg_flatten = batch.trg[1:].view(-1)\n",
    "            loss = criterion(outputs_flatten, trg_flatten)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 11s\n",
      "\tTrain Loss: 4.695 | Train PPL: 109.423\n",
      "\t Val. Loss: 4.899 |  Val. PPL: 134.123\n",
      "Epoch: 02 | Time: 0m 11s\n",
      "\tTrain Loss: 4.641 | Train PPL: 103.672\n",
      "\t Val. Loss: 4.873 |  Val. PPL: 130.759\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "N_EPOCHS = 2\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'slt_libras.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('slt_libras.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.873590140216119"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.874 | Test PPL: 130.790 |\n"
     ]
    }
   ],
   "source": [
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, libras, portuguese, device, max_length=50):\n",
    "\n",
    "    if type(sentence) == str:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    tokens.insert(0, libras.init_token)\n",
    "    tokens.append(libras.eos_token)\n",
    "    text_to_indices = [libras.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [portuguese.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == portuguese.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [portuguese.vocab.itos[idx] for idx in outputs]\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6207, 32)\n",
       "    (rnn): LSTM(32, 64, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(9046, 32)\n",
       "    (rnn): LSTM(32, 64, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=64, out_features=9046, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence:  ['ter', 'frente', 'caminho', 'longo']\n",
      "Correct translation:  ['e', 'temos', 'um', 'longo', 'caminho', 'a', 'nossa', 'frente']\n"
     ]
    }
   ],
   "source": [
    "print(\"Test sentence: \", test_data[1].src)\n",
    "print(\"Correct translation: \", test_data[1].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\"eu férias julho planejar viajar europa.\"]\n",
    "correct_sentence = [\"eu tenho férias em julho e vou planejar uma viagem `a europa.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence:  ['e', 'o', '<unk>', '<unk>', '<unk>', '<unk>', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Translated sentence: \", translate_sentence(model, test_sentence, libras_sentences_source, portuguese_sentences_target, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  médico ter paciência bom\n",
      "target sentence:  meu médico é bom tem paciência .\n"
     ]
    }
   ],
   "source": [
    "example_idx = 5\n",
    "example = val_data.examples[example_idx]\n",
    "print('source sentence: ', ' '.join(example.src))\n",
    "print('target sentence: ', ' '.join(example.trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1, 9046])"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tensor = libras_sentences_source.process([example.src]).to(device)\n",
    "trg_tensor = portuguese_sentences_target.process([example.trg]).to(device)\n",
    "print(trg_tensor.shape)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(src_tensor, trg_tensor, teacher_forcing_ratio=0)\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o <unk> é o que o <unk> .'"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "' '.join([portuguese_sentences_target.vocab.itos[idx] for idx in output_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448d780881a940d49ab191b2d1b463a8c02ae47a3b8017bde1517a5fe99f211"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
