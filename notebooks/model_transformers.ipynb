{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "portuguese_df = pd.read_csv('../data/pt_br.txt', sep='\\t', header=None, names=['example_portuguese_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with manual corrections\n",
    "data_cleaned = pd.read_csv('../data/libras_dictionary.csv')\n",
    "data_cleaned[\"source\"] = \"ines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>subject</th>\n",
       "      <th>interpretation</th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "      <th>example_libras_sentence</th>\n",
       "      <th>grammar_class</th>\n",
       "      <th>word_origin</th>\n",
       "      <th>video_link</th>\n",
       "      <th>image_link</th>\n",
       "      <th>hand_image_link</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Primeira letra do alfabeto da língua portugues...</td>\n",
       "      <td>Invente qualquer palavra que comece com a letr...</td>\n",
       "      <td>VOCÊ INVENTAR QUALQUER PALAVRA COMEÇAR A.</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABACATE</td>\n",
       "      <td>FRUTA</td>\n",
       "      <td>O fruto do abacateiro. Comestível, tem a polpa...</td>\n",
       "      <td>Você gosta de abacate com leite?</td>\n",
       "      <td>VOCÊ GOSTAR ABACATE LEITE JUNTO?</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABACAXI</td>\n",
       "      <td>FRUTA</td>\n",
       "      <td>Fruta de casca grossa e áspera. Sua polpa pode...</td>\n",
       "      <td>Hoje tomei suco de abacaxi, ele estava ácido.</td>\n",
       "      <td>HOJE S-U-C-O ABACAXI BEBER ÁCID@.</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABAFAR</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Cobrir ou fechar, para manter o calor.</td>\n",
       "      <td>Se você quer abafar seu quarto, é melhor fecha...</td>\n",
       "      <td>S-I VOCÊ QUERER QUARTO SE@ ABAFAR A-R? MELHOR ...</td>\n",
       "      <td>VERBO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABAIXO</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Lugar, posição ou situação inferior, em relaçã...</td>\n",
       "      <td>Não é no primeiro apartamento abaixo, é no seg...</td>\n",
       "      <td>APARTAMENTO PRIMEIR@ NÃO SEGUND@ ABAIXO.</td>\n",
       "      <td>ADV.</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABAIXO-ASSINADO</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Lista encaminhada a determinado destinatário, ...</td>\n",
       "      <td>Os surdos fizeram um abaixo-assinado pedindo m...</td>\n",
       "      <td>SURD@ ABAIXO-ASSINADO PEDIR GOVERNO EMPREGO MAIS.</td>\n",
       "      <td>LOC. SUBST.</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABAJUR</td>\n",
       "      <td>CASA</td>\n",
       "      <td>Quebra-luz; peça usada para oferecer iluminaçã...</td>\n",
       "      <td>A lâmpada do abajur queimou.</td>\n",
       "      <td>coisa-cônicaLÂMPADA LÂMPADA-QUEIMAR .</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ABANAR</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Agitar seguidamente um leque ou abano.</td>\n",
       "      <td>Está vendo aquela velha se abanando? Ela é avó...</td>\n",
       "      <td>2sOLHAR3s VELH@ ABANAR-LEQUE LÁ(ME) V-O-V-O AM...</td>\n",
       "      <td>VERBO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABANDONAR1</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Desprezar, largar, deixar de cuidar.</td>\n",
       "      <td>O carro velho foi abandonado naquela garagem.</td>\n",
       "      <td>AQUEL@ CARAGEM CARRO VELH@ ABANDONAR.</td>\n",
       "      <td>VERBO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABANDONAR2</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Deixar pessoa ou algum objeto, sem intenção de...</td>\n",
       "      <td>Coitada dessa criança tão bonita dormindo na r...</td>\n",
       "      <td>CRIANÇA BONIT@ DORMIR R-U-A COITAD@! P-A-I M-Ã...</td>\n",
       "      <td>VERBO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ABARROTADO</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Muito cheio; ocupado acima da capacidade normal.</td>\n",
       "      <td>Ontem o metrô estava abarrotado de gente.</td>\n",
       "      <td>ONTEM METRÔ PESSOA MUIT@ ABARROTAD@ pessoaCHEI@</td>\n",
       "      <td>ADJETIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ABASTADO</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Possuidor de bens; rico; endinheirado.</td>\n",
       "      <td>Ele é rico, abastado.</td>\n",
       "      <td>EL@ RIC@ ABASTAD@.</td>\n",
       "      <td>ADJETIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ABATIDO</td>\n",
       "      <td>SENTIMENTOS</td>\n",
       "      <td>Deprimido; desanimado.</td>\n",
       "      <td>A família consolou a viúva que estava abatida.</td>\n",
       "      <td>FAMÍLIA CONSOLAR MULHER VIÚV@ ABATID@</td>\n",
       "      <td>ADJETIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ABATIMENTO</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Desconto; redução.</td>\n",
       "      <td>Você pode fazer abatimento dos gastos hospital...</td>\n",
       "      <td>HOSPITAL GASTAR VOCÊ I.R. ABATIMENTO PODER.</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ABDICAR</td>\n",
       "      <td>NENHUM</td>\n",
       "      <td>Ato voluntário de renúncia a um cargo conferid...</td>\n",
       "      <td>O Coronel abdicou de tornar-se major do exército.</td>\n",
       "      <td>HOMEM AQUEL@ CORONEL ABDICAR MAJOR EXERCITO.</td>\n",
       "      <td>VERBO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ABDÔMEN</td>\n",
       "      <td>CORPO</td>\n",
       "      <td>Região entre o tórax e a bacia, em cuja cavida...</td>\n",
       "      <td>O médico me mandou tirar raio-x do abdômen.</td>\n",
       "      <td>MÉDIC@ 3sMANDAR1s RAIO-X ABDÔMEN.</td>\n",
       "      <td>SUBSTANTIVO</td>\n",
       "      <td>Nacional</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>https://www.ines.gov.br/dicionario-de-libras/p...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word      subject  \\\n",
       "0                 A       NENHUM   \n",
       "1           ABACATE        FRUTA   \n",
       "2           ABACAXI        FRUTA   \n",
       "3            ABAFAR       NENHUM   \n",
       "4            ABAIXO       NENHUM   \n",
       "5   ABAIXO-ASSINADO       NENHUM   \n",
       "6            ABAJUR         CASA   \n",
       "7            ABANAR       NENHUM   \n",
       "8        ABANDONAR1       NENHUM   \n",
       "9        ABANDONAR2       NENHUM   \n",
       "10       ABARROTADO       NENHUM   \n",
       "11         ABASTADO       NENHUM   \n",
       "12          ABATIDO  SENTIMENTOS   \n",
       "13       ABATIMENTO       NENHUM   \n",
       "14          ABDICAR       NENHUM   \n",
       "15          ABDÔMEN        CORPO   \n",
       "\n",
       "                                       interpretation  \\\n",
       "0   Primeira letra do alfabeto da língua portugues...   \n",
       "1   O fruto do abacateiro. Comestível, tem a polpa...   \n",
       "2   Fruta de casca grossa e áspera. Sua polpa pode...   \n",
       "3              Cobrir ou fechar, para manter o calor.   \n",
       "4   Lugar, posição ou situação inferior, em relaçã...   \n",
       "5   Lista encaminhada a determinado destinatário, ...   \n",
       "6   Quebra-luz; peça usada para oferecer iluminaçã...   \n",
       "7              Agitar seguidamente um leque ou abano.   \n",
       "8                Desprezar, largar, deixar de cuidar.   \n",
       "9   Deixar pessoa ou algum objeto, sem intenção de...   \n",
       "10   Muito cheio; ocupado acima da capacidade normal.   \n",
       "11             Possuidor de bens; rico; endinheirado.   \n",
       "12                             Deprimido; desanimado.   \n",
       "13                                 Desconto; redução.   \n",
       "14  Ato voluntário de renúncia a um cargo conferid...   \n",
       "15  Região entre o tórax e a bacia, em cuja cavida...   \n",
       "\n",
       "                          example_portuguese_sentence  \\\n",
       "0   Invente qualquer palavra que comece com a letr...   \n",
       "1                    Você gosta de abacate com leite?   \n",
       "2       Hoje tomei suco de abacaxi, ele estava ácido.   \n",
       "3   Se você quer abafar seu quarto, é melhor fecha...   \n",
       "4   Não é no primeiro apartamento abaixo, é no seg...   \n",
       "5   Os surdos fizeram um abaixo-assinado pedindo m...   \n",
       "6                        A lâmpada do abajur queimou.   \n",
       "7   Está vendo aquela velha se abanando? Ela é avó...   \n",
       "8       O carro velho foi abandonado naquela garagem.   \n",
       "9   Coitada dessa criança tão bonita dormindo na r...   \n",
       "10          Ontem o metrô estava abarrotado de gente.   \n",
       "11                              Ele é rico, abastado.   \n",
       "12     A família consolou a viúva que estava abatida.   \n",
       "13  Você pode fazer abatimento dos gastos hospital...   \n",
       "14  O Coronel abdicou de tornar-se major do exército.   \n",
       "15        O médico me mandou tirar raio-x do abdômen.   \n",
       "\n",
       "                              example_libras_sentence grammar_class  \\\n",
       "0           VOCÊ INVENTAR QUALQUER PALAVRA COMEÇAR A.   SUBSTANTIVO   \n",
       "1                    VOCÊ GOSTAR ABACATE LEITE JUNTO?   SUBSTANTIVO   \n",
       "2                   HOJE S-U-C-O ABACAXI BEBER ÁCID@.   SUBSTANTIVO   \n",
       "3   S-I VOCÊ QUERER QUARTO SE@ ABAFAR A-R? MELHOR ...         VERBO   \n",
       "4            APARTAMENTO PRIMEIR@ NÃO SEGUND@ ABAIXO.          ADV.   \n",
       "5   SURD@ ABAIXO-ASSINADO PEDIR GOVERNO EMPREGO MAIS.   LOC. SUBST.   \n",
       "6               coisa-cônicaLÂMPADA LÂMPADA-QUEIMAR .   SUBSTANTIVO   \n",
       "7   2sOLHAR3s VELH@ ABANAR-LEQUE LÁ(ME) V-O-V-O AM...         VERBO   \n",
       "8               AQUEL@ CARAGEM CARRO VELH@ ABANDONAR.         VERBO   \n",
       "9   CRIANÇA BONIT@ DORMIR R-U-A COITAD@! P-A-I M-Ã...         VERBO   \n",
       "10    ONTEM METRÔ PESSOA MUIT@ ABARROTAD@ pessoaCHEI@      ADJETIVO   \n",
       "11                                 EL@ RIC@ ABASTAD@.      ADJETIVO   \n",
       "12              FAMÍLIA CONSOLAR MULHER VIÚV@ ABATID@      ADJETIVO   \n",
       "13        HOSPITAL GASTAR VOCÊ I.R. ABATIMENTO PODER.   SUBSTANTIVO   \n",
       "14       HOMEM AQUEL@ CORONEL ABDICAR MAJOR EXERCITO.         VERBO   \n",
       "15                  MÉDIC@ 3sMANDAR1s RAIO-X ABDÔMEN.   SUBSTANTIVO   \n",
       "\n",
       "   word_origin                                         video_link  \\\n",
       "0     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "1     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "2     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "3     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "4     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "5     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "6     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "7     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "8     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "9     Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "10    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "11    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "12    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "13    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "14    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "15    Nacional  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "\n",
       "                                           image_link  \\\n",
       "0   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "1   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "2   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "3   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "4   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "5   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "6   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "7   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "8   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "9   https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "10  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "11  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "12  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "13  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "14  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "15  https://www.ines.gov.br/dicionario-de-libras/p...   \n",
       "\n",
       "                                      hand_image_link source  \n",
       "0   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "1   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "2   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "3   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "4   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "5   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "6   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "7   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "8   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "9   https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "10  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "11  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "12  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "13  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "14  https://www.ines.gov.br/dicionario-de-libras/p...   ines  \n",
       "15  https://www.ines.gov.br/dicionario-de-libras/p...   ines  "
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean words: 8.71071071071071\n",
      "Mean characters: 14\n"
     ]
    }
   ],
   "source": [
    "# Get the mean size of the sentences\n",
    "\n",
    "print(f\"Mean words: {data_cleaned['example_portuguese_sentence'].str.split().str.len().mean()}\")\n",
    "print(f\"Mean characters: {data_cleaned['example_portuguese_sentence'].str.len().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the sentences with less than 50 characters\n",
    "\n",
    "portuguese_df = portuguese_df[(portuguese_df['example_portuguese_sentence'].str.len() <= 50) & (portuguese_df['example_portuguese_sentence'].str.split().str.len() >= 8)]\n",
    "portuguese_df[\"source\"] = \"opus_nlpl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_cleaned[[\"example_portuguese_sentence\", \"source\"]], portuguese_df[[\"example_portuguese_sentence\", \"source\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_portuguese_sentence</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invente qualquer palavra que comece com a letr...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Você gosta de abacate com leite?</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoje tomei suco de abacaxi, ele estava ácido.</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Se você quer abafar seu quarto, é melhor fecha...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Não é no primeiro apartamento abaixo, é no seg...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Se o freio estiver com problema, é perigoso o ...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Aquele homem tem muita acne no rosto.</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A janela da minha casa é de aço, difícil de qu...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Aquele homem açoitou o cavalo para que acelera...</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Meu açoite está velho.</td>\n",
       "      <td>ines</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          example_portuguese_sentence source\n",
       "0   Invente qualquer palavra que comece com a letr...   ines\n",
       "1                    Você gosta de abacate com leite?   ines\n",
       "2       Hoje tomei suco de abacaxi, ele estava ácido.   ines\n",
       "3   Se você quer abafar seu quarto, é melhor fecha...   ines\n",
       "4   Não é no primeiro apartamento abaixo, é no seg...   ines\n",
       "..                                                ...    ...\n",
       "95  Se o freio estiver com problema, é perigoso o ...   ines\n",
       "96              Aquele homem tem muita acne no rosto.   ines\n",
       "97  A janela da minha casa é de aço, difícil de qu...   ines\n",
       "98  Aquele homem açoitou o cavalo para que acelera...   ines\n",
       "99                             Meu açoite está velho.   ines\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optmi\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.utils.data import random_split, TensorDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from pprint import pprint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "import spacy\n",
    "\n",
    "# Seeding for reproducible results everytime\n",
    "SEED = 777\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Preprocessing\n",
    "\n",
    "Let's see some of the process it can do,\n",
    "\n",
    "* Train/ Valid/ Test Split: partition your data into a specified train/ valid/ test set.\n",
    "\n",
    "* File Loading: load the text corpus of various formats (.txt,.json,.csv).\n",
    "\n",
    "* Tokenization: breaking sentences into list of words.\n",
    "\n",
    "* Vocab: Generate a list of vocabulary from the text corpus.\n",
    "\n",
    "* Words to Integer Mapper: Map words into integer numbers for the entire corpus and vice versa.\n",
    "\n",
    "* Word Vector: Convert a word from higher dimension to lower dimension (Word Embedding).\n",
    "\n",
    "* Batching: Generate batches of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy portuguese tokenizer\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(clean_text(data['example_portuguese_sentence'][15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o o DET det\n",
      "médico médico NOUN nsubj\n",
      "me eu PRON obj\n",
      "mandou mandar VERB ROOT\n",
      "tirar tirar VERB xcomp\n",
      "raio raio NOUN obj\n",
      "x x VERB obj\n",
      "do de o ADP case\n",
      "abdômen Abdômen NOUN iobj\n",
      ". . PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "médico NOUN nsubj\n",
      "eu PRON obj\n",
      "mandar VERB ROOT\n",
      "tirar VERB xcomp\n",
      "raio NOUN obj\n",
      "x VERB obj\n",
      "Abdômen NOUN iobj\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.pos_ == 'VERB'\\\n",
    "        or token.pos_ == 'NOUN' \\\n",
    "        or token.pos_ == 'ADJ' \\\n",
    "        or token.pos_ == 'ADV' \\\n",
    "        or token.pos_ == 'NUM' \\\n",
    "        or token.pos_ == 'PROPN' \\\n",
    "        or token.pos_ == 'PRON' \\\n",
    "        or token.pos_ == 'SCONJ':\n",
    "        print(token.lemma_, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ontem']"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_adverbs(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize sentences in subject, verb, object\n",
    "\n",
    "def get_subjects(doc):\n",
    "\n",
    "    subjects = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj':\n",
    "            subjects.append(token.lemma_)\n",
    "    return subjects\n",
    "\n",
    "def get_verbs(doc):\n",
    "    \n",
    "    verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            verbs.append(token.lemma_)\n",
    "    return verbs\n",
    "\n",
    "def get_objects(doc):\n",
    "\n",
    "    objects = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'obj' or token.dep_ == 'dobj' or token.dep_ == 'iobj':\n",
    "            objects.append(token.lemma_)\n",
    "    return objects\n",
    "\n",
    "\n",
    "def get_nouns(doc):\n",
    "\n",
    "    nouns = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            nouns.append(token.lemma_)\n",
    "    return nouns\n",
    "\n",
    "def get_adjectives(doc):\n",
    "\n",
    "    adjectives = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            adjectives.append(token.lemma_)\n",
    "    return adjectives\n",
    "\n",
    "def get_adverbs(doc):\n",
    "\n",
    "    adverbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADV':\n",
    "            adverbs.append(token.lemma_)\n",
    "    return adverbs\n",
    "\n",
    "def get_numbers(doc):\n",
    "\n",
    "    numbers = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NUM':\n",
    "            numbers.append(token.lemma_)\n",
    "    return numbers\n",
    "\n",
    "\n",
    "def get_conjunctions(doc):\n",
    "\n",
    "    conjunctions = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'SCONJ':\n",
    "            conjunctions.append(token.lemma_)\n",
    "    return conjunctions\n",
    "\n",
    "def get_pronouns(doc):\n",
    "\n",
    "    pronouns = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            pronouns.append(token.lemma_)\n",
    "    return pronouns\n",
    "\n",
    "def get_proper_nouns(doc):\n",
    "\n",
    "    proper_nouns = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            proper_nouns.append(token.lemma_)\n",
    "    return proper_nouns\n",
    "\n",
    "\n",
    "def organize_sentence_svo(doc):\n",
    "    sentence = []\n",
    "    sentence.append(get_subjects(doc))\n",
    "    sentence.append(get_verbs(doc))\n",
    "    sentence.append(get_objects(doc))\n",
    "    sentence.append(get_nouns(doc))\n",
    "    sentence.append(get_adjectives(doc))\n",
    "    sentence.append(get_adverbs(doc))\n",
    "    sentence.append(get_numbers(doc))\n",
    "    sentence.append(get_conjunctions(doc))\n",
    "    sentence.append(get_pronouns(doc))\n",
    "    sentence.append(get_proper_nouns(doc))\n",
    "\n",
    "    # remove duplicates and empty lists\n",
    "\n",
    "    sentence = [list(set(x)) for x in sentence if x != []]\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['médico'],\n",
       " ['mandar', 'tirar', 'raio-x'],\n",
       " ['eu', 'raio-x'],\n",
       " ['médico', 'abdômen'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['eu'],\n",
       " []]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organize_sentence(nlp(data['example_portuguese_sentence'][15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oblique nominal'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('obl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove characters that are not letters\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-ú.!?]+\", ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\" +\", ' ', text)\n",
    "\n",
    "    # Remove extra spaces at the beginning and end of the sentence\n",
    "    text = text.strip()\n",
    "\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(text):\n",
    "\n",
    "    text = clean_text(text)\n",
    "\n",
    "    return [tok.text for tok in spacy_pt.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "libras_sentences_source = Field(\n",
    "    tokenize = spacy_tokenizer,\n",
    "    lower=True,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")\n",
    "\n",
    "portuguese_sentences_target = Field(\n",
    "    tokenize = spacy_tokenizer,\n",
    "    lower=True,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TabularDataset(path='../data/libras_dictionary_cleaned.csv', format='csv', fields=[('trg', portuguese_sentences_target), ('src', libras_sentences_source)], skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = dataset.split(split_ratio=[0.8, 0.1, 0.1], random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4622\n",
      "Number of validation examples: 577\n",
      "Number of testing examples: 578\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tain sentence example - Libras  ['lembrar', 'nós', 'dinheiro', 'dívida', 'sdevolver', 's', 'agora', '.']\n",
      "Tain sentence example - Portuguese  ['lembra', 'do', 'que', 'você', 'me', 'deve', '?', 'devolva', 'me', 'agora', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tain sentence example - Libras \", train_data.examples[0].src)\n",
    "print(\"Tain sentence example - Portuguese \", train_data.examples[0].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "libras_sentences_source.build_vocab(train_data, min_freq=2)\n",
    "portuguese_sentences_target.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (libras) vocabulary: 2019\n",
      "Unique tokens in target (portuguse) vocabulary: 2572\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (libras) vocabulary: {len(libras_sentences_source.vocab)}\")\n",
    "print(f\"Unique tokens in target (portuguse) vocabulary: {len(portuguese_sentences_target.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - source batch\n",
    "    Layer : \n",
    "        source batch -> Embedding -> LSTM\n",
    "    Output :\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    input_dim : int\n",
    "        Input dimension, should equal to the source vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, emb_dim: int, hid_dim: int, n_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src_batch: torch.LongTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        src_batch : 2d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [sent len, batch size].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hidden, cell : 3d torch.LongTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(src_batch) # [sent len, batch size, emb dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs -> [sent len, batch size, hidden dim * n directions]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "        - first token in the target batch\n",
    "        - LSTM hidden state from the encoder\n",
    "        - LSTM cell state from the encoder\n",
    "    Layer :\n",
    "        target batch -> Embedding -- \n",
    "                                   |\n",
    "        encoder hidden state ------|--> LSTM -> Linear\n",
    "                                   |\n",
    "        encoder cell state   -------\n",
    "        \n",
    "    Output :\n",
    "        - prediction\n",
    "        - LSTM hidden state\n",
    "        - LSTM cell state\n",
    "\n",
    "    Parmeters\n",
    "    ---------\n",
    "    output : int\n",
    "        Output dimension, should equal to the target vocab size.\n",
    "    \n",
    "    emb_dim : int\n",
    "        Embedding layer's dimension.\n",
    "        \n",
    "    hid_dim : int\n",
    "        LSTM Hidden/Cell state's dimension.\n",
    "        \n",
    "    n_layers : int\n",
    "        Number of LSTM layers.\n",
    "        \n",
    "    dropout : float\n",
    "        Dropout for the LSTM layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim: int, emb_dim: int, hid_dim: int, n_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "    def forward(self, trg: torch.LongTensor, hidden: torch.FloatTensor, cell: torch.FloatTensor):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trg : 1d torch.LongTensor\n",
    "            Batched tokenized source sentence of shape [batch size].\n",
    "            \n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction : 2d torch.LongTensor\n",
    "            For each token in the batch, the predicted target vobulary.\n",
    "            Shape [batch size, output dim]\n",
    "\n",
    "        hidden, cell : 3d torch.FloatTensor\n",
    "            Hidden and cell state of the LSTM layer. Each state's shape\n",
    "            [n layers * n directions, batch size, hidden dim]\n",
    "        \"\"\"\n",
    "        # [1, batch size, emb dim], the 1 serves as sent len\n",
    "        embedded = self.embedding(trg.unsqueeze(0))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.out(outputs.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            'Hidden dimensions of encoder and decoder must be equal!'\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'Encoder and decoder must have equal number of layers!'\n",
    "\n",
    "    def forward(self, src_batch: torch.LongTensor, trg_batch: torch.LongTensor,\n",
    "                teacher_forcing_ratio: float=0.5):\n",
    "\n",
    "        max_len, batch_size = trg_batch.shape\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # tensor to store decoder's output\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # last hidden & cell state of the encoder is used as the decoder's initial hidden state\n",
    "        hidden, cell = self.encoder(src_batch)\n",
    "\n",
    "        trg = trg_batch[0]\n",
    "        for i in range(1, max_len):\n",
    "            prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
    "            outputs[i] = prediction\n",
    "\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                trg = trg_batch[i]\n",
    "            else:\n",
    "                trg = prediction.argmax(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([23, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([22, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([24, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([21, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([27, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([24, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([21, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([21, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([21, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([18, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([21, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([16, 14])\n",
      "torch.Size([24, 14])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([12, 32])\n",
      "torch.Size([21, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([20, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([11, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([6, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([19, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([13, 32])\n",
      "torch.Size([24, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([17, 32])\n",
      "torch.Size([9, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([22, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([16, 32])\n",
      "torch.Size([7, 32])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([8, 32])\n",
      "torch.Size([15, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(batch.src.shape)\n",
    "    print(batch.trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(libras_sentences_source.vocab)\n",
    "OUTPUT_DIM = len(portuguese_sentences_target.vocab)\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = libras_sentences_source.vocab.stoi[libras_sentences_source.pad_token]\n",
    "TRG_PAD_IDX = portuguese_sentences_target.vocab.stoi[portuguese_sentences_target.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 430,828 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(2019, 32)\n",
       "    (rnn): LSTM(32, 64, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2572, 32)\n",
       "    (rnn): LSTM(32, 64, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=64, out_features=2572, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seq2seq, iterator, optimizer, criterion):\n",
    "    seq2seq.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = seq2seq(batch.src, batch.trg)\n",
    "\n",
    "        # 1. as mentioned in the seq2seq section, we will\n",
    "        # cut off the first element when performing the evaluation\n",
    "        # 2. the loss function only works on 2d inputs\n",
    "        # with 1d targets we need to flatten each of them\n",
    "        outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        trg_flatten = batch.trg[1:].view(-1)\n",
    "        loss = criterion(outputs_flatten, trg_flatten)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(seq2seq, iterator, criterion):\n",
    "    seq2seq.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # turn off teacher forcing\n",
    "            outputs = seq2seq(batch.src, batch.trg, teacher_forcing_ratio=0) \n",
    "\n",
    "            # trg = [trg sent len, batch size]\n",
    "            # output = [trg sent len, batch size, output dim]\n",
    "            outputs_flatten = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            trg_flatten = batch.trg[1:].view(-1)\n",
    "            loss = criterion(outputs_flatten, trg_flatten)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 1s\n",
      "\tTrain Loss: 6.185 | Train PPL: 485.519\n",
      "\t Val. Loss: 5.094 |  Val. PPL: 163.110\n",
      "Epoch: 02 | Time: 0m 1s\n",
      "\tTrain Loss: 5.258 | Train PPL: 192.042\n",
      "\t Val. Loss: 5.005 |  Val. PPL: 149.148\n",
      "Epoch: 03 | Time: 0m 1s\n",
      "\tTrain Loss: 5.191 | Train PPL: 179.572\n",
      "\t Val. Loss: 4.957 |  Val. PPL: 142.168\n",
      "Epoch: 04 | Time: 0m 1s\n",
      "\tTrain Loss: 5.142 | Train PPL: 171.006\n",
      "\t Val. Loss: 4.924 |  Val. PPL: 137.607\n",
      "Epoch: 05 | Time: 0m 1s\n",
      "\tTrain Loss: 5.096 | Train PPL: 163.384\n",
      "\t Val. Loss: 4.908 |  Val. PPL: 135.308\n",
      "Epoch: 06 | Time: 0m 1s\n",
      "\tTrain Loss: 5.054 | Train PPL: 156.583\n",
      "\t Val. Loss: 4.873 |  Val. PPL: 130.738\n",
      "Epoch: 07 | Time: 0m 1s\n",
      "\tTrain Loss: 5.008 | Train PPL: 149.564\n",
      "\t Val. Loss: 4.854 |  Val. PPL: 128.309\n",
      "Epoch: 08 | Time: 0m 1s\n",
      "\tTrain Loss: 4.963 | Train PPL: 142.958\n",
      "\t Val. Loss: 4.849 |  Val. PPL: 127.590\n",
      "Epoch: 09 | Time: 0m 1s\n",
      "\tTrain Loss: 4.922 | Train PPL: 137.231\n",
      "\t Val. Loss: 4.843 |  Val. PPL: 126.869\n",
      "Epoch: 10 | Time: 0m 1s\n",
      "\tTrain Loss: 4.883 | Train PPL: 131.993\n",
      "\t Val. Loss: 4.851 |  Val. PPL: 127.824\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'slt_libras.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('slt_libras.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.703183701163844"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.703 | Test PPL: 110.298 |\n"
     ]
    }
   ],
   "source": [
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, libras, portuguese, device, max_length=50):\n",
    "\n",
    "    if type(sentence) == str:\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    tokens.insert(0, libras.init_token)\n",
    "    tokens.append(libras.eos_token)\n",
    "    text_to_indices = [libras.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [portuguese.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == portuguese.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [portuguese.vocab.itos[idx] for idx in outputs]\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(2019, 32)\n",
       "    (rnn): LSTM(32, 64, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2572, 32)\n",
       "    (rnn): LSTM(32, 64, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=64, out_features=2572, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence:  ['eu', 'férias', 'julho', 'planejar', 'viajar', 'europa', '.']\n",
      "Correct translation:  ['eu', 'tenho', 'férias', 'em', 'julho', 'e', 'vou', 'planejar', 'uma', 'viagem', 'à', 'europa', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Test sentence: \", test_data[1].src)\n",
    "print(\"Correct translation: \", test_data[1].trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\"eu férias julho planejar viajar europa.\"]\n",
    "correct_sentence = [\"eu tenho férias em julho e vou planejar uma viagem `a europa.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence:  ['o', '<unk>', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(\"Translated sentence: \", translate_sentence(model, test_sentence, libras_sentences_source, portuguese_sentences_target, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence:  gostar ler fábula .\n",
      "target sentence:  eu gosto de ler fábulas .\n"
     ]
    }
   ],
   "source": [
    "example_idx = 5\n",
    "example = val_data.examples[example_idx]\n",
    "print('source sentence: ', ' '.join(example.src))\n",
    "print('target sentence: ', ' '.join(example.trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 2572])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tensor = libras_sentences_source.process([example.src]).to(device)\n",
    "trg_tensor = portuguese_sentences_target.process([example.trg]).to(device)\n",
    "print(trg_tensor.shape)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(src_tensor, trg_tensor, teacher_forcing_ratio=0)\n",
    "\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o <unk> <unk> <unk> . <eos> <eos>'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
    "' '.join([portuguese_sentences_target.vocab.itos[idx] for idx in output_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1448d780881a940d49ab191b2d1b463a8c02ae47a3b8017bde1517a5fe99f211"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
